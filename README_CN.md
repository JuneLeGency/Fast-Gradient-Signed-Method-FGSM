# 人工智能对抗攻击演示案例：FGSM

## 1. 图形化界面 (GUI) 用法 (推荐)

为了方便教学演示，本项目已封装为图形化应用。

### **如何启动GUI**

1.  **激活虚拟环境**：
    首先，请确保您在 `Fast-Gradient-Signed-Method-FGSM` 目录下打开终端，并激活虚拟环境：
    ```bash
    source .venv/bin/activate
    ```

2.  **运行主程序**：
    接着，运行 `app_gui.py` 脚本来启动程序：
    ```bash
    python app_gui.py
    ```

### **界面功能说明**

应用启动后，您会看到一个包含三个选项卡的窗口：

1.  **正常识别选项卡**：
    -   点击 **“选择图片”** 按钮，可以从 `images/` 目录或其他位置选择一张图片。
    -   程序会自动对图片进行识别，并在下方文本框中显示置信度最高的前5个结果。

2.  **非定向攻击 (FGSM) 选项卡**：
    -   此功能固定攻击 `images/panda.jpg` 图片。
    -   拖动 **“扰动强度 (Epsilon)”** 滑块可以选择不同的攻击强度。
    -   点击 **“开始攻击”** 按钮，界面会同时展示“原始图像”、“扰动”和“对抗样本”三张图，并在下方显示攻击前后的识别结果对比。

3.  **定向攻击选项卡**：
    -   此功能目前正在开发中。

---

## 2. 案例简介

本项目旨在通过一个经典的对抗攻击案例，向学生普及“人工智能内生安全”的基本概念。我们使用的攻击方法是**快速梯度符号攻击（Fast Gradient Sign Method, FGSM）**，这是由 Ian Goodfellow 等人在2014年提出的、最早也是最著名的对抗攻击算法之一。

**核心思想**：神经网络通过梯度下降法来调整权重、最小化损失函数，从而学习识别图像。FGSM 的思想则巧妙地“反其道而行之”：它不调整模型权重，而是利用模型对于输入图像的梯度信息，对原始输入图像进行微小的、人眼难以察觉的修改，以达到**最大化损失函数**的目的。这样生成的“对抗样本”在人类看来与原图几乎没有区别，但却能让一个训练有素的神经网络模型做出完全错误的分类判断。

本案例将通过以下三个步骤，完整地展示这一过程：

1.  **正常识别**：证明我们使用的预训练模型在正常情况下是有效的，能够准确识别图片。
2.  **非定向攻击**：对原始图片施加微小扰动，使得模型无法识别出原始类别，而是给出一个完全错误的答案。
3.  **定向攻击**：对原始图片施加经过优化的扰动，使得模型不仅识别错误，而且会以极高的置信度将图片识别为我们指定的任意目标类别（例如，将“熊猫”识别为“咖啡杯”）。

通过这个直观的案例，学生可以深刻理解到人工智能模型的脆弱性，并对“AI安全”建立初步的认识。

---

## 3. 环境准备

本项目代码基于 Python 和 PyTorch。我们已经为您配置好了所有必要的脚本和环境管理。

1.  **创建与激活环境**：
    首先，请在 `Fast-Gradient-Signed-Method-FGSM` 目录下打开终端，并创建一个新的虚拟环境：
    ```bash
    python3 -m venv .venv
    ```
    然后激活该环境：
    ```bash
    source .venv/bin/activate
    ```

2.  **安装依赖** (推荐方式):
    本项目使用 `pyproject.toml` 管理依赖。请使用 `uv`（或 `pip`）通过以下命令在项目根目录一键安装所有依赖：
    ```bash
    uv pip install .
    ```

3.  **中文字体**：为了在结果图片中正确显示中文，项目中已包含“阿里巴巴普惠体”字体文件 (`Alibaba-PuHuiTi-Medium.ttf`)，并已在代码中配置加载，无需额外安装。

---

## 4. 文件结构说明

```
Fast-Gradient-Signed-Method-FGSM/
├── images/                     # 存放用于测试的原始图片
├── .venv/                      # Python 虚拟环境
├── app_gui.py                  # 图形化界面 (GUI) 的主程序
├── attack_core.py              # 封装了核心攻击与识别逻辑的模块
├── predict_normal.py           # (命令行) 用于演示正常的图像识别
├── fgsm.py                     # (命令行) 用于演示非定向的FGSM攻击
├── targeted_fgsm.py            # (命令行) 用于演示定向的FGSM攻击
├── utils.py                    # (命令行) 辅助工具脚本，包含绘图等函数
├── imagenet_class_index_cn.json # ImageNet 1000个类别的中文翻译
├── Alibaba-PuHuiTi-Medium.ttf  # 用于显示中文的字体文件
└── README_CN.md                # 本说明文档
```

---

## 5. 高级用法：命令行

如果您或您的学生熟悉命令行，也可以直接运行原始的脚本文件来分步进行演示。

### 步骤一：正常图像识别 (证明模型有效)

`predict_normal.py` 脚本可以接受一个图片路径作为参数。如果不提供参数，它将默认识别 `images/panda.jpg`。

1.  **识别默认的熊猫图片**：
    ```bash
    python predict_normal.py
    ```

2.  **识别其他图片**：
    ```bash
    python predict_normal.py images/dog.jpeg
    ```

### 步骤二：非定向攻击 (让模型犯错)

`fgsm.py` 脚本将对熊猫图片进行非定向攻击。

```bash
python fgsm.py
```

### 步骤三：定向攻击 (指鹿为马)

`targeted_fgsm.py` 脚本的目标是让模型把“大熊猫”识别为“咖啡杯”。

```bash
python targeted_fgsm.py
```
